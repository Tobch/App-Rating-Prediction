{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6019e697",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning & Preprocessing (`clean.py` and tryout.ipynb)\n",
    "\n",
    "## **Why Clean and Preprocess Data?**\n",
    "Machine learning models require:\n",
    "- **Numerical input** (no text or categorical data).\n",
    "- **No missing values** (NaNs).\n",
    "- **Consistent feature formats** (e.g., all sizes in MB).\n",
    "- **No outliers or erroneous data** (e.g., typos, impossible values).\n",
    "\n",
    "### **Step-by-Step Breakdown**\n",
    "\n",
    "#### **a. Column Renaming**\n",
    "- **Purpose:** Makes the data human-readable and easier to work with.\n",
    "- **Example:** `X0` → `AppName`, `X1` → `Category`, etc.\n",
    "\n",
    "#### **b. Dropping Unnecessary Columns**\n",
    "- **Purpose:** Removes columns that do not help prediction (e.g., `AppName` is just an identifier).\n",
    "\n",
    "#### **c. Category Handling**\n",
    "- **Remove Erroneous Categories:** E.g., a category labeled `'1.9'` is likely a data entry error.\n",
    "- **Group Rare Categories:** Categories with very few samples can cause overfitting. Grouping them into `'OTHER'` ensures the model doesn't learn noise.\n",
    "- **One-Hot Encoding:** Converts each category into a binary column (e.g., `Cat_BUSINESS` = 1 if the app is business, else 0). This allows models to use categorical data.\n",
    "\n",
    "#### **d. Numeric Conversion**\n",
    "- **NumReviews, AppSize, NumInstalls, Price:** All must be numeric.\n",
    "- **AppSize:** Converts all sizes to MB (e.g., `12k` → `0.0117 MB`, `20M` → `20 MB`). Handles missing or ambiguous values by filling with the median.\n",
    "- **NumInstalls:** Removes `+` and commas, converts to integer.\n",
    "- **Price:** Ensures all prices are numeric.\n",
    "\n",
    "#### **e. Boolean and Categorical Encoding**\n",
    "- **IsFree:** Converts `\"Free\"` to 0 and `\"Paid\"` to 1.\n",
    "- **AgeCategory:** One-hot encodes age restrictions (e.g., `Age_Everyone`, `Age_Teen`).\n",
    "- **Genres:** Apps can have multiple genres (e.g., `\"Action;Adventure\"`). Uses `MultiLabelBinarizer` to create a column for each genre, set to 1 if the app has that genre.\n",
    "\n",
    "#### **f. Date Handling**\n",
    "- **LastUpdate:** Converts to datetime, extracts the year (e.g., `2018`), then drops the original column.\n",
    "\n",
    "#### **g. Dropping More Columns**\n",
    "- **Version, MinAndroidVer:** Often too granular or inconsistent for modeling, so they are dropped.\n",
    "\n",
    "#### **h. Handling Missing Values**\n",
    "- **AppSize:** Fill with median.\n",
    "- **Rating (target):** Drop rows with missing ratings (since you can't train on them).\n",
    "- **Other features:** Ensure no missing values remain.\n",
    "\n",
    "#### **i. Normalization**\n",
    "- **NumInstalls, NumReviews:** These are often highly skewed (some apps have millions of installs, most have few). Applying `np.log1p()` (logarithm of 1 + value) compresses large values and spreads out small ones, making the data easier for models to learn from.\n",
    "\n",
    "#### **j. Final Checks**\n",
    "- **No NaNs:** Ensures all missing values are handled.\n",
    "- **No text columns:** All features must be numeric for ML models.\n",
    "- **No infinite values:** Ensures no division-by-zero or log(0) errors.\n",
    "\n",
    "#### **k. Saving**\n",
    "- **Cleaned Data:** Saved for use in training and testing.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Model Training & Evaluation (`Train.py`)\n",
    "\n",
    "## **Why Train Multiple Models?**\n",
    "- **No single model is best for all problems.**\n",
    "- **Trying a variety of models** (linear, tree-based, instance-based, etc.) helps find the best fit for your data.\n",
    "\n",
    "### **Step-by-Step Breakdown**\n",
    "\n",
    "#### **a. Data Loading**\n",
    "- Loads the cleaned, normalized data for training, validation, and testing.\n",
    "\n",
    "#### **b. Feature Selection**\n",
    "- Selects only the columns used for prediction (excludes the target `Rating` and any identifiers).\n",
    "\n",
    "#### **c. Target Extraction**\n",
    "- Sets the `Rating` column as the value to predict.\n",
    "\n",
    "#### **d. Missing Value Checks**\n",
    "- Ensures no missing values in features or targets.\n",
    "\n",
    "#### **e. LazyML (LazyPredict)**\n",
    "- **What is it?** A library that quickly trains and evaluates many regression models with default settings.\n",
    "- **Why use it?** To get a fast, broad comparison of many algorithms and see which ones are promising.\n",
    "- **How does it work?** It fits each model on the training data and evaluates on the validation set, reporting metrics like R² and RMSE.\n",
    "\n",
    "#### **f. K-Fold Cross-Validation**\n",
    "- **What is it?** A robust way to estimate model performance.\n",
    "- **How does it work?**\n",
    "  - Splits the training data into `k` (e.g., 5) folds.\n",
    "  - Trains the model on `k-1` folds, tests on the remaining fold.\n",
    "  - Repeats this `k` times, each time with a different test fold.\n",
    "  - Reports the average performance.\n",
    "- **Why use it?** Reduces the risk of overfitting to a particular train/validation split and gives a more reliable estimate of model performance.\n",
    "\n",
    "#### **g. Model Training & Evaluation**\n",
    "- **Trains each model on the full training set.**\n",
    "- **Evaluates on validation and test sets using Mean Squared Error (MSE).**\n",
    "- **Saves each trained model for later use.**\n",
    "\n",
    "---\n",
    "\n",
    "## **Models Used:**\n",
    "\n",
    "### **1. LinearRegression**\n",
    "- **How it works:** Finds the best-fitting straight line (hyperplane) through the data.\n",
    "- **When to use:** When you suspect a linear relationship between features and target.\n",
    "- **Pros:** Simple, interpretable.\n",
    "- **Cons:** Can't capture non-linear relationships.\n",
    "\n",
    "### **2. Ridge Regression**\n",
    "- **How it works:** Like LinearRegression, but adds L2 regularization (penalizes large coefficients).\n",
    "- **Why:** Helps prevent overfitting, especially when features are correlated.\n",
    "\n",
    "### **3. Lasso Regression**\n",
    "- **How it works:** Like Ridge, but uses L1 regularization (can shrink some coefficients to zero, effectively selecting features).\n",
    "- **Why:** Useful for feature selection and preventing overfitting.\n",
    "\n",
    "### **4. RandomForestRegressor**\n",
    "- **How it works:** Builds many decision trees on random subsets of the data and averages their predictions.\n",
    "- **Why:** Handles non-linearities, interactions, and is robust to outliers and overfitting.\n",
    "\n",
    "### **5. GradientBoostingRegressor**\n",
    "- **How it works:** Builds trees sequentially, each one correcting the errors of the previous.\n",
    "- **Why:** Often achieves high accuracy, especially on tabular data.\n",
    "\n",
    "### **6. KNeighborsRegressor**\n",
    "- **How it works:** Predicts the target by averaging the values of the k nearest neighbors in feature space.\n",
    "- **Why:** Simple, non-parametric, can capture local patterns.\n",
    "\n",
    "### **7. SVR (Support Vector Regression)**\n",
    "- **How it works:** Tries to fit as many data points as possible within a margin, using kernel tricks to capture non-linear relationships.\n",
    "- **Why:** Good for complex, non-linear data, robust to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "## **Evaluation Metrics**\n",
    "\n",
    "### **Mean Squared Error (MSE)**\n",
    "- **Definition:** The average of the squared differences between predicted and actual values.\n",
    "- **Why:** Penalizes large errors more than small ones, commonly used for regression.\n",
    "\n",
    "### **Cross-Validation MSE**\n",
    "- **Definition:** The average MSE across all folds in K-Fold CV.\n",
    "- **Why:** Gives a more robust estimate of model performance.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Prediction (`prediction.py`)\n",
    "\n",
    "## **Purpose**\n",
    "- **Apply a trained model to new, unseen data** (e.g., for a competition or real-world deployment).\n",
    "\n",
    "### **Step-by-Step Breakdown**\n",
    "\n",
    "#### **a. Load Test Data**\n",
    "- Reads the cleaned and normalized test data.\n",
    "\n",
    "#### **b. Feature Alignment**\n",
    "- Ensures the test data has the same features as the training data.\n",
    "- Handles missing columns by filling with zeros (so the model can still make predictions).\n",
    "\n",
    "#### **c. Load Model**\n",
    "- Loads a previously trained model (e.g., Lasso, RandomForest) using `joblib`.\n",
    "\n",
    "#### **d. Predict**\n",
    "- Uses the model to predict ratings for the test data.\n",
    "\n",
    "#### **e. Save Results**\n",
    "- Outputs predictions to a CSV file for submission or further analysis.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Interactive Data Cleaning (`tryout.ipynb`)\n",
    "\n",
    "## **Purpose**\n",
    "- **Prototype and visualize** each cleaning step.\n",
    "- **Debug and explore** the data interactively.\n",
    "- **Document** the cleaning process.\n",
    "\n",
    "### **Why Use a Notebook?**\n",
    "- You can see the effect of each transformation.\n",
    "- Easy to plot, summarize, and check data at each step.\n",
    "- Useful for developing and testing your cleaning pipeline before scripting it in clean.py.\n",
    "\n",
    "---\n",
    "\n",
    "# **How Everything Fits Together**\n",
    "\n",
    "1. **Raw Data** → **`clean.py`/`tryout.ipynb`** → **Cleaned Data**\n",
    "2. **Cleaned Data** → **`Train.py`** → **Trained Models**\n",
    "3. **Trained Models + New Data** → **`prediction.py`** → **Predictions**\n",
    "\n",
    "---\n",
    "\n",
    "# **Why This Pipeline?**\n",
    "\n",
    "- **Data cleaning** ensures the models get the best possible input.\n",
    "- **Trying multiple models** increases the chance of finding the best fit for your data.\n",
    "- **Cross-validation** ensures your results are robust and not due to chance.\n",
    "- **Saving models** allows for easy deployment and reuse.\n",
    "- **Automated prediction** enables you to apply your solution to new data quickly.\n",
    "\n",
    "---\n",
    "\n",
    "# **Summary Table**\n",
    "\n",
    "| File             | Purpose                                      | Key Steps/Models Used                                                                 |\n",
    "|------------------|----------------------------------------------|---------------------------------------------------------------------------------------|\n",
    "| clean.py       | Data cleaning & preprocessing                | Renaming, encoding, normalization, missing value handling                             |\n",
    "| Train.py       | Model training, validation, evaluation       | Linear, Ridge, Lasso, RandomForest, GradientBoosting, KNN, SVR, LazyML, K-Fold CV     |\n",
    "| prediction.py  | Predicting on new/test data                  | Loads model, aligns features, predicts, saves results                                 |\n",
    "| tryout.ipynb   | Interactive data cleaning & exploration      | Step-by-step cleaning, encoding, normalization, splitting, saving                     |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
